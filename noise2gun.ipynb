{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Imports"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "source": [
    "# !conda activate n2v\n",
    "\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "import sys\n",
    "import torch\n",
    "import random\n",
    "import zarr\n",
    "from PIL import Image\n",
    "from skimage import data\n",
    "from skimage import filters\n",
    "from skimage import metrics\n",
    "\n",
    "from funlib.learn.torch.models import UNet, ConvPass\n",
    "import gunpowder as gp\n",
    "\n",
    "# from this repo\n",
    "import loser\n",
    "from boilerPlate import BoilerPlate\n",
    "from segway.tasks.make_zarr_from_tiff import task_make_zarr_from_tiff_volume as tif2zarr"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "2021-08-06 13:17:39.979981: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudart.so.10.1\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Set Parameters (including data source, training variables, destination, etc.)"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## **Paths for training, predictions and results**\n",
    "\n",
    "**`train_source:`:** This is the path to your folders containing the Training_source (noisy images). To find the path of the folder containing your datasets, go to your Files on the left of the notebook, navigate to the folder containing your files and copy the path by right-clicking on the folder, **Copy path** and pasting it into the right box below.\n",
    "\n",
    "**`model_name`:** Use only my_model -style, not my-model (Use \"_\" not \"-\"). Do not use spaces in the name. Do not re-use the name of an existing model (saved in the same folder), otherwise it will be overwritten.\n",
    "\n",
    "**`model_path`**: Enter the path where your model will be saved once trained (for instance your result folder).\n",
    "\n",
    "\n",
    "## **Training parameters**\n",
    "\n",
    "**`num_epochs`:** Input how many epochs (rounds) the network will be trained. Preliminary results can already be observed after a few (10-30) epochs, but a full training should run for 100-200 epochs. Evaluate the performance after training (see 5.). **Default value: 30**\n",
    "\n",
    "**`side_length`:** Noise2Void divides the image into patches for training. Input the size of the patches (length of a side). The value should be smaller than the dimensions of the image and divisible by 8. **Default value: 100**\n",
    "\n",
    "**If you get an Out of memory (OOM) error during the training, manually decrease the patch_size values until the OOM error disappear.**\n",
    "\n",
    "**`batch_size:`** This parameter defines the number of patches seen in each training step. Noise2Void requires a large batch size for stable training. Reduce this parameter if your GPU runs out of memory. **Default value: 1**\n",
    "\n",
    "**`num_steps`:** Define the number of training steps by epoch. By default this parameter is calculated so that each image / patch is seen at least once per epoch. **Default value: Number of patch / batch_size**\n",
    "\n",
    "**`perc_validation`:**  Input the percentage of your training dataset you want to use to validate the network during the training. **Default value: 10** \n",
    "\n",
    "**`init_learn_rate`:** Input the initial value to be used as learning rate. **Default value: 0.0004**\n",
    "\n",
    "**`perc_hotPixels:`** Percent of output pixels to designate as targets and *heat* for training **Default value: 0.198**\n"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "source": [
    "train_source = '/n/groups/htem/ESRF_id16a/tomo_ML/ReducedAnglesXray/CARE/mCTX/450p_stacks/mCTX_17keV_30nm_512c_first256.tif' #EXPECTS TIFF VOLUME\n",
    "#TODO: MAKE ABLE TO HANDLE TIFF VOLUME OR STACK\n",
    "\n",
    "data_name = 'mCTX_17keV_30nm_512c_first256'\n",
    "data_path = '/n/groups/htem/ESRF_id16a/tomo_ML/ReducedAnglesXray/CARE/mCTX/450p_stacks/'\n",
    "data_format = 'tif'\n",
    "\n",
    "model_name = 'noise2gun_mCTX30nm_450p'\n",
    "model_path = ''\n",
    "voxel_size=[30, 30, 30] # set for each dataset (may be able to get from zarr)\n",
    "\n",
    "side_length = 12 # in voxels for prediction (i.e. network output) - actual used ROI for network input will be bigger for valid padding\n",
    "unet_depth = 4 # number of layers in unet\n",
    "downsample_factor = 2\n",
    "conv_padding = 'valid'\n",
    "num_fmaps = 12\n",
    "fmap_inc_factor=5\n",
    "perc_hotPixels = 0.198\n",
    "constant_upsample=True\n",
    "\n",
    "num_epochs = 100\n",
    "batch_size = 1\n",
    "num_steps = 100\n",
    "perc_validation = 10\n",
    "init_learn_rate = 0.0004"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Make sure data source is a **zarr** "
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "if data_format != 'zarr':\n",
    "    img = Image.open(train_source)\n",
    "    size = np.array([img.n_frames, *img.size])\n",
    "    \n",
    "    output_dataset = 'volumes/train'\n",
    "    output_file = data_path + data_name + '.zarr'\n",
    "\n",
    "\n",
    "    # aligned_dir_path=\"/n/groups/htem/data/qz/200121_B2_final\"\n",
    "    # output_file='/n/groups/htem/data/qz/200121_B2_final.n5'\n",
    "    # output_dataset=\"volumes/raw\"\n",
    "    # voxel_size='40 4 4'#zyx\n",
    "    # roi_offset='3000 2048 2048'\n",
    "    # ### volume size calculation\n",
    "    # # Z: 100*40 = 4000\n",
    "    # # Y: 1504*4 = 6144\n",
    "    # # X: 1504*4 = 6144\n",
    "    # roi_shape='4000 6144 6144'\n",
    "    # y_tile_size=data.shape[1]\n",
    "    # x_tile_size=data.shape[0]\n",
    "    # section_dir_name_format=\"left_resliced{:05d}.tif\"\n",
    "    $aligned_dir_path $y_tile_size $x_tile_size $voxel_size $output_file $output_dataset --roi_offset $roi_offset --roi_shape $roi_shape --section_dir_name_format $section_dir_name_format --single_file_format 1\n",
    "    tif2zarr.make_zarr_from_tiff(...)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Build Gunpowder Pipeline for Training\n",
    "\n",
    "### Elements are:\n",
    "\n",
    "- Data Source\n",
    "- *(optional) Normalization*\n",
    "- Random Patch Grab\n",
    "- Pixel Heating (select and mutate *hotPixels*, i.e. training targets, and keep masks)\n",
    "- Simple Augmentation (rotations/reflections)\n",
    "- Stacking\n",
    "- Training\n"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# declare arrays to use in the pipeline\n",
    "raw = gp.ArrayKey('RAW') # raw data\n",
    "hot = gp.ArrayKey('HOT') # data with random pixels heated\n",
    "mask = gp.ArrayKey('MASK') # data with random pixels heated\n",
    "prediction = gp.ArrayKey('PREDICTION') # prediction of denoised data\n",
    "\n",
    "source = gp.ZarrSource(    # add the data source\n",
    "    train_source,  # the zarr container\n",
    "    {raw: 'raw'},  # which dataset to associate to the array key\n",
    "    {raw: gp.ArraySpec(interpolatable=True)}  # meta-information\n",
    ")\n",
    "\n",
    "# add normalization\n",
    "# normalize = gp.Normalize(raw)\n",
    "\n",
    "# add a RandomLocation node to the pipeline to randomly select a sample\n",
    "random_location = gp.RandomLocation()\n",
    "\n",
    "# add transpositions/reflections\n",
    "simple_augment = gp.SimpleAugment()\n",
    "\n",
    "# stack for batches\n",
    "stack = gp.Stack(batch_size)\n",
    "\n",
    "# add pixel heater\n",
    "boilerPlate = BoilerPlate(raw, mask, hot, plate_size=side_length, perc_hotPixels=perc_hotPixels)\n",
    "\n",
    "# define our network model for training\n",
    "unet = UNet(\n",
    "  in_channels=1,\n",
    "  num_fmaps=num_fmaps,\n",
    "  fmap_inc_factor=fmap_inc_factor,\n",
    "  downsample_factors=[[downsample_factor,]*3,] * (unet_depth - 1),\n",
    "  padding=conv_padding,\n",
    "  constant_upsample=constant_upsample,\n",
    "  voxel_size=voxel_size # set for each dataset\n",
    "  )\n",
    "\n",
    "model = torch.nn.Sequential(\n",
    "  unet,\n",
    "  ConvPass(num_fmaps, 1, [(1, 1, 1)], activation=None),\n",
    "  torch.nn.Sigmoid())\n",
    "\n",
    "# pick loss function\n",
    "loss = loser.MaskedMSELoss()\n",
    "\n",
    "# pick optimizer\n",
    "optimizer = torch.optim.Adam(model.parameters())\n",
    "\n",
    "# create a train node using our model, loss, and optimizer\n",
    "train = gp.torch.Train(\n",
    "  model,\n",
    "  loss,\n",
    "  optimizer,\n",
    "  inputs = {\n",
    "    'input': hot\n",
    "  },\n",
    "  loss_inputs = {\n",
    "    'input': prediction,\n",
    "    'mask': mask,\n",
    "    'target': raw\n",
    "  },\n",
    "  outputs = {\n",
    "    0: prediction\n",
    "  })\n",
    "\n",
    "# figure out proper ROI padding for context\n",
    "conv_passes = 2 # set by default in unet\n",
    "kernel_size = 3 # set by default in unet\n",
    "context_side_length = 2 * np.sum([(conv_passes * (kernel_size - 1)) * (2 ** scale) for scale in np.arange(unet_depth - 1)]) + (conv_passes * (kernel_size - 1)) * (2 ** (unet_depth - 1)) + side_length\n",
    "\n",
    "# create request\n",
    "request = gp.BatchRequest()\n",
    "request[raw] = gp.Roi(tuple(0*np.array(voxel_size)), tuple(context_side_length*np.array(voxel_size)))\n",
    "request[hot] = gp.Roi(tuple(0*np.array(voxel_size)), tuple(context_side_length*np.array(voxel_size)))\n",
    "request[mask] = gp.Roi(tuple(0*np.array(voxel_size)), tuple(context_side_length*np.array(voxel_size)))\n",
    "request[prediction] = gp.Roi(tuple(0*np.array(voxel_size)), tuple(side_length*np.array(voxel_size)))\n",
    "\n",
    "# assemble pipeline\n",
    "pipeline = (source +\n",
    "            random_location +\n",
    "            simple_augment + \n",
    "            stack + \n",
    "            boilerPlate +\n",
    "            train)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Train"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Examine Results"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Build Prediction Pipeline"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [],
   "metadata": {}
  }
 ],
 "metadata": {
  "orig_nbformat": 4,
  "language_info": {
   "name": "python",
   "version": "3.7.10",
   "mimetype": "text/x-python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "pygments_lexer": "ipython3",
   "nbconvert_exporter": "python",
   "file_extension": ".py"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.7.10 64-bit ('n2v': conda)"
  },
  "interpreter": {
   "hash": "f9cb357f91f63a353a1c2ecd5237cebe3ac17167e08747c30b5061e5be51817e"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}