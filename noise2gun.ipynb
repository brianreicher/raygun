{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Imports & Other Setup"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "source": [
    "# !conda activate n2v\n",
    "%load_ext autoreload\n",
    "\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "import sys\n",
    "import torch\n",
    "import random\n",
    "import zarr\n",
    "from PIL import Image\n",
    "from skimage import data\n",
    "from skimage import filters\n",
    "from skimage import metrics\n",
    "import os\n",
    "\n",
    "from funlib.learn.torch.models import UNet, ConvPass\n",
    "import gunpowder as gp\n",
    "import logging\n",
    "#logging.basicConfig(level=logging.INFO)\n",
    "\n",
    "# from this repo\n",
    "import loser\n",
    "from boilerPlate import BoilerPlate\n",
    "# from segway.tasks.make_zarr_from_tiff import task_make_zarr_from_tiff_volume as tif2zarr"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "2021-08-16 18:07:43.713751: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcudart.so.11.0\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "source": [
    "#Pick a GPU or 2\n",
    "torch.cuda.set_device(3)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "source": [
    "def imshow(raw, ground_truth=None, prediction=None):\n",
    "  fig, axes = plt.subplots(1, 3, figsize=(50,150))\n",
    "  if len(raw.shape) == 3:\n",
    "    axes[0].imshow(raw[0])\n",
    "  else:\n",
    "    axes[0].imshow(raw)\n",
    "  col = 1\n",
    "  if ground_truth is not None:\n",
    "    if len(ground_truth.shape) == 3:\n",
    "      axes[col].imshow(ground_truth[0])\n",
    "    else:\n",
    "      axes[col].imshow(ground_truth)\n",
    "    col += 1\n",
    "  if prediction is not None:\n",
    "    if prediction.size < raw.size:\n",
    "      pads = np.subtract(raw.shape, prediction.shape) // 2\n",
    "      pad_tuple = []\n",
    "      for p in pads:\n",
    "          pad_tuple.append((p, p))\n",
    "      prediction = np.pad(prediction, tuple(pad_tuple))\n",
    "    if len(prediction.shape) == 3:\n",
    "      axes[col].imshow(prediction[0])\n",
    "    else:\n",
    "      axes[col].imshow(prediction)\n",
    "  plt.show()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Set Parameters (including data source, training variables, destination, etc.)"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## **Paths for training, predictions and results**\n",
    "\n",
    "**`train_source:`:** This is the path to your folders containing the Training_source (noisy images). To find the path of the folder containing your datasets, go to your Files on the left of the notebook, navigate to the folder containing your files and copy the path by right-clicking on the folder, **Copy path** and pasting it into the right box below.\n",
    "\n",
    "**`model_name`:** Use only my_model -style, not my-model (Use \"_\" not \"-\"). Do not use spaces in the name. Do not re-use the name of an existing model (saved in the same folder), otherwise it will be overwritten.\n",
    "\n",
    "**`model_path`**: Enter the path where your model will be saved once trained (for instance your result folder).\n",
    "\n",
    "\n",
    "## **Training parameters**\n",
    "\n",
    "**`num_epochs`:** Input how many epochs (rounds) the network will be trained. Preliminary results can already be observed after a few (10-30) epochs, but a full training should run for 100-200 epochs. Evaluate the performance after training (see 5.). **Default value: 30**\n",
    "\n",
    "**`side_length`:** Noise2Void divides the image into patches for training. Input the size of the patches (length of a side). The value should be smaller than the dimensions of the image and divisible by 8. **Default value: 100**\n",
    "\n",
    "**If you get an Out of memory (OOM) error during the training, manually decrease the patch_size values until the OOM error disappear.**\n",
    "\n",
    "**`batch_size:`** This parameter defines the number of patches seen in each training step. Noise2Void requires a large batch size for stable training. Reduce this parameter if your GPU runs out of memory. **Default value: 1**\n",
    "\n",
    "**`num_steps`:** Define the number of training steps by epoch. By default this parameter is calculated so that each image / patch is seen at least once per epoch. **Default value: Number of patch / batch_size**\n",
    "\n",
    "**`perc_validation`:**  Input the percentage of your training dataset you want to use to validate the network during the training. **Default value: 10** \n",
    "\n",
    "**`init_learn_rate`:** Input the initial value to be used as learning rate. **Default value: 0.0004**\n",
    "\n",
    "**`perc_hotPixels:`** Percent of output pixels to designate as targets and *heat* for training **Default value: 0.198**\n"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "source": [
    "train_source = '/n/groups/htem/ESRF_id16a/tomo_ML/ReducedAnglesXray/CARE/mCTX/450p_stacks/mCTX_17keV_30nm_512c_first256.zarr' #EXPECTS TIFF VOLUME\n",
    "#TODO: MAKE ABLE TO HANDLE TIFF VOLUME OR STACK\n",
    "\n",
    "data_name = 'mCTX_17keV_30nm_512c_first256'\n",
    "data_path = '/n/groups/htem/ESRF_id16a/tomo_ML/ReducedAnglesXray/CARE/mCTX/450p_stacks/'\n",
    "data_format = 'zarr'\n",
    "\n",
    "tensorboard_path = '/n/groups/htem/ESRF_id16a/tomo_ML/ReducedAnglesXray/raygun/n2g/tensorboard/'\n",
    "model_name = 'noise2gun_mCTX30nm_450p'\n",
    "model_path = '/n/groups/htem/ESRF_id16a/tomo_ML/ReducedAnglesXray/raygun/n2g/models/'\n",
    "os.chdir(model_path)\n",
    "voxel_size=[30, 30, 30] # set for each dataset (may be able to get from zarr)\n",
    "\n",
    "side_length = 8#12 # in voxels for prediction (i.e. network output) - actual used ROI for network input will be bigger for valid padding\n",
    "unet_depth = 4 # number of layers in unet\n",
    "downsample_factor = 2\n",
    "conv_padding = 'valid'\n",
    "num_fmaps = 12\n",
    "fmap_inc_factor=5\n",
    "perc_hotPixels = 0.198\n",
    "constant_upsample=True\n",
    "\n",
    "num_epochs = 10000\n",
    "batch_size = 5\n",
    "num_steps = 100\n",
    "perc_validation = 10\n",
    "init_learn_rate = 0.0004"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Make sure data source is a **zarr** "
   ],
   "metadata": {}
  },
  {
   "cell_type": "raw",
   "source": [
    "#NEEDS TO BE MADE\n",
    "\n",
    "if data_format != 'zarr':\n",
    "\n",
    "#CURRENT SCRIPT (BASH)\n",
    "# go to working directory\n",
    "cd /n/groups/htem/ESRF_id16a/tomo_ML/ReducedAnglesXray/CARE/mCTX/450p_stacks\n",
    "# cd /n/groups/htem/Segmentation/shared-nondev/cbx_fn\n",
    "\n",
    "prefix='mCTX_17keV_30nm_512c_first256'\n",
    "aligned_dir_path=\"/n/groups/htem/ESRF_id16a/tomo_ML/ReducedAnglesXray/CARE/mCTX/450p_stacks/${prefix}\"\n",
    "output_file=\"/n/groups/htem/ESRF_id16a/tomo_ML/ReducedAnglesXray/CARE/mCTX/450p_stacks/${prefix}.zarr\"\n",
    "output_dataset=\"volumes/train\"\n",
    "voxel_size='30 30 30'\n",
    "roi_offset='0 0 0'\n",
    "roi_shape='7680 15360 15360'\n",
    "x_tile_size=512\n",
    "y_tile_size=512\n",
    "section_dir_name_format=\"${prefix}_{:04d}.tif\"\n",
    "max_voxel_count=262144 # means chunk size = 256 * 1024 (recommended for NeuroGlancer chunk loading)\n",
    "\n",
    "python /n/groups/htem/users/jlr54/raygun/segway/tasks/make_zarr_from_tiff/task_make_zarr_from_tiff_single_file.py $aligned_dir_path $y_tile_size $x_tile_size $voxel_size $output_file $output_dataset --roi_offset $roi_offset --roi_shape $roi_shape --section_dir_name_format $section_dir_name_format --single_file_format 1 --max_voxel_count $max_voxel_count --no_launch_workers 0 --overwrite 0 --num_workers 4"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Build Gunpowder Pipeline for Training\n",
    "\n",
    "### Elements are:\n",
    "\n",
    "- Data Source\n",
    "- *(optional) Normalization*\n",
    "- Random Patch Grab\n",
    "- Pixel Heating (select and mutate *hotPixels*, i.e. training targets, and keep masks)\n",
    "- Simple Augmentation (rotations/reflections)\n",
    "- Stacking\n",
    "- Training\n"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "source": [
    "%autoreload\n",
    "from boilerPlate import BoilerPlate\n",
    "import loser"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "source": [
    "# declare arrays to use in the pipeline\n",
    "raw = gp.ArrayKey('RAW') # raw data\n",
    "hot = gp.ArrayKey('HOT') # data with random pixels heated\n",
    "mask = gp.ArrayKey('MASK') # data with random pixels heated\n",
    "prediction = gp.ArrayKey('PREDICTION') # prediction of denoised data\n",
    "\n",
    "source = gp.ZarrSource(    # add the data source\n",
    "    train_source,  # the zarr container\n",
    "    {raw: 'volumes/train'},  # which dataset to associate to the array key\n",
    "    {raw: gp.ArraySpec(interpolatable=True)}  # meta-information\n",
    ")\n",
    "\n",
    "# add normalization\n",
    "# normalize = gp.Normalize(raw)\n",
    "\n",
    "# add a RandomLocation node to the pipeline to randomly select a sample\n",
    "random_location = gp.RandomLocation()\n",
    "\n",
    "# add transpositions/reflections\n",
    "simple_augment = gp.SimpleAugment()\n",
    "\n",
    "# stack for batches\n",
    "stack = gp.Stack(batch_size)\n",
    "\n",
    "# add pixel heater\n",
    "boilerPlate = BoilerPlate(raw, mask, hot, plate_size=side_length, perc_hotPixels=perc_hotPixels, ndims=3)\n",
    "\n",
    "# prepare tensors for UNet\n",
    "unsqueeze = gp.Unsqueeze([hot, mask, raw])\n",
    "\n",
    "# setup a cache\n",
    "cache = gp.PreCache()\n",
    "\n",
    "# define our network model for training\n",
    "unet = UNet(\n",
    "  in_channels=1,\n",
    "  num_fmaps=num_fmaps,\n",
    "  fmap_inc_factor=fmap_inc_factor,\n",
    "  downsample_factors=[(downsample_factor,)*3,] * (unet_depth - 1),\n",
    "  padding=conv_padding,\n",
    "  constant_upsample=constant_upsample,\n",
    "  voxel_size=voxel_size # set for each dataset\n",
    "  )\n",
    "\n",
    "model = torch.nn.Sequential(\n",
    "  unet,\n",
    "  ConvPass(num_fmaps, 1, [(1, 1, 1)], activation=None),\n",
    "  torch.nn.Sigmoid())\n",
    "\n",
    "# pick loss function\n",
    "#loss = loser.MaskedMSELoss()\n",
    "loss = loser.maskedMSE\n",
    "#loss = loser.lossFunctionN2V\n",
    "\n",
    "# pick optimizer\n",
    "optimizer = torch.optim.Adam(model.parameters())\n",
    "\n",
    "# create a train node using our model, loss, and optimizer\n",
    "train = gp.torch.Train(\n",
    "  model,\n",
    "  loss,\n",
    "  optimizer,\n",
    "  inputs = {\n",
    "    'input': hot\n",
    "  },\n",
    "  loss_inputs = {\n",
    "    'src': prediction,\n",
    "    'mask': mask,\n",
    "    'target': raw\n",
    "  },\n",
    "  outputs = {\n",
    "    0: prediction\n",
    "  },\n",
    "  log_dir=tensorboard_path,\n",
    "  log_every=100,\n",
    "  checkpoint_basename=model_name,\n",
    "  #save_every=100\n",
    "  )\n",
    "\n",
    "# figure out proper ROI padding for context\n",
    "conv_passes = 2 # set by default in unet\n",
    "kernel_size = 3 # set by default in unet\n",
    "context_side_length = 2 * np.sum([(conv_passes * (kernel_size - 1)) * (2 ** level) for level in np.arange(unet_depth - 1)]) + (conv_passes * (kernel_size - 1)) * (2 ** (unet_depth - 1)) + (conv_passes * (kernel_size - 1)) + side_length\n",
    "\n",
    "# create request\n",
    "request = gp.BatchRequest()\n",
    "request[raw] = gp.Roi(tuple(0*np.array(voxel_size)), tuple(context_side_length*np.array(voxel_size)))\n",
    "request[mask] = gp.Roi(tuple(0*np.array(voxel_size)), tuple(context_side_length*np.array(voxel_size)))\n",
    "request[hot] = gp.Roi(tuple(0*np.array(voxel_size)), tuple(context_side_length*np.array(voxel_size)))\n",
    "request[prediction] = gp.Roi(tuple(0*np.array(voxel_size)), tuple(side_length*np.array(voxel_size)))\n",
    "\n",
    "# get performance stats\n",
    "performance = gp.PrintProfilingStats(every=100)\n",
    "\n",
    "# assemble pipeline\n",
    "pipeline = (source +\n",
    "            #normalize + \n",
    "            random_location +\n",
    "            simple_augment + \n",
    "            boilerPlate +\n",
    "            unsqueeze + \n",
    "            cache +\n",
    "            stack + \n",
    "            train +\n",
    "            performance\n",
    "            )"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Train"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "source": [
    "with gp.build(pipeline):\n",
    "  for i in range(num_epochs):\n",
    "    batch = pipeline.request_batch(request)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Examine Results"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "#Check results\n",
    "imshow(batch[raw].data.squeeze(), batch[hot].data.squeeze(), batch[prediction].data.squeeze())"
   ],
   "outputs": [
    {
     "output_type": "error",
     "ename": "NameError",
     "evalue": "name 'imshow' is not defined",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_31323/3750315167.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m#Check results\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mimshow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mraw\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mhot\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mprediction\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'imshow' is not defined"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Build Prediction Pipeline"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Scratch work and trouble shooting"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "source": [
    "%autoreload\n",
    "from boilerPlate import BoilerPlate\n",
    "import loser"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "source": [
    "a = torch.tensor(np.random.rand(1,3))\n",
    "b = torch.tensor(np.random.rand(1,3)) > 0.5\n",
    "c = torch.tensor(np.random.rand(1,3)) \n",
    "a1 = a[b]\n",
    "c1 = c[b]\n",
    "test = loss(a, b, c)\n",
    "this = test.backward()\n",
    "this.numel()"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "1\n"
     ]
    },
    {
     "output_type": "error",
     "ename": "AttributeError",
     "evalue": "'NoneType' object has no attribute 'numel'",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_5362/3723161695.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0mtest\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mb\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0mthis\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtest\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m \u001b[0mthis\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m: 'NoneType' object has no attribute 'numel'"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "source": [
    "import torch.nn.functional as F\n",
    "from torch import nn\n",
    "from loser import *"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "source": [
    "def my_loss(output, target):\n",
    "    loss = torch.mean((output - target)**2)\n",
    "    return loss\n",
    "\n",
    "model = nn.Linear(2, 2)\n",
    "x = torch.randn(1, 2)\n",
    "target = torch.randn(1, 2)\n",
    "output = model(x)\n",
    "#loss = my_loss(output, target)\n",
    "loss = lossFunctionN2V()\n",
    "loss.backward()\n",
    "print(model.weight.grad)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "/mnt/orchestra_nfs/users/jlr54/envs/miniconda3/envs/n2v/lib/python3.7/site-packages/torch/nn/_reduction.py:43: UserWarning: size_average and reduce args will be deprecated, please use reduction='mean' instead.\n",
      "  warnings.warn(warning.format(ret))\n"
     ]
    },
    {
     "output_type": "error",
     "ename": "AttributeError",
     "evalue": "'MaskedMSELoss' object has no attribute 'backward'",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_34619/3802717623.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;31m#loss = my_loss(output, target)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mloser\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mMaskedMSELoss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgrad\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/mnt/orchestra_nfs/users/jlr54/envs/miniconda3/envs/n2v/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__getattr__\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m    574\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mmodules\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    575\u001b[0m         raise AttributeError(\"'{}' object has no attribute '{}'\".format(\n\u001b[0;32m--> 576\u001b[0;31m             type(self).__name__, name))\n\u001b[0m\u001b[1;32m    577\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    578\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__setattr__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'MaskedMSELoss' object has no attribute 'backward'"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "source": [
    "test = tuple([torch.tensor(0),torch.tensor(0),torch.tensor(1),torch.tensor(1)])\n",
    "test[::-1]"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "(tensor(1), tensor(1), tensor(0), tensor(0))"
      ]
     },
     "metadata": {},
     "execution_count": 26
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [],
   "outputs": [],
   "metadata": {}
  }
 ],
 "metadata": {
  "orig_nbformat": 4,
  "language_info": {
   "name": "python",
   "version": "3.7.10",
   "mimetype": "text/x-python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "pygments_lexer": "ipython3",
   "nbconvert_exporter": "python",
   "file_extension": ".py"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.7.10 64-bit ('n2v': conda)"
  },
  "interpreter": {
   "hash": "f9cb357f91f63a353a1c2ecd5237cebe3ac17167e08747c30b5061e5be51817e"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}